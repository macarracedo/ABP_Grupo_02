{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ABPNetbook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macarracedo/ABP_Grupo_02/blob/main/ABPNetbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKTUPU8yFUa1"
      },
      "source": [
        "#Twinder: Sistema de clasificaci칩n\n",
        "\n",
        "Hemos entrenado 2 SVMs a partir de 2 datasets distintos. El primero de ellos se trata de reviews de usuarios en Amazon.com, y est치n etiquetados en 2 posibles valores (positivo y negativo).\n",
        "\n",
        "\n",
        "El segundo de los datasets, conocido como sentiment140, consiste en publicaciones de twitter con 2 etiquetas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS4zT7AYLVge"
      },
      "source": [
        "# Primer DS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PoFMXG122oL"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import nltk\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "#Max 3600000\n",
        "trainingValues=10000\n",
        "\n",
        "#Max 400000\n",
        "testingValues=100"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqOVvyo83Qdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381145f4-249f-46a7-8830-734b6104dadb"
      },
      "source": [
        "f = open(\"/content/drive/MyDrive/ESEI/7 Cuatrimestre/ABP/train.ft.txt\", \"r\")\n",
        "lines = f.readlines()\n",
        "trainingData_aws = pd.DataFrame(columns=['label','text'])\n",
        "print(len(lines))\n",
        "i=0\n",
        "for line in lines:\n",
        "  i+=1\n",
        "  #print(\"\\t\",i)\n",
        "  label = line.split(' ',1)[0]\n",
        "  if label == \"__label__1\":\n",
        "    label=\"0\"\n",
        "  elif label == \"__label__2\":\n",
        "    label=\"4\"\n",
        "  #print(label)\n",
        "  text = line.split(' ',1)[1]\n",
        "  #print(text)\n",
        "  trainingData_aws = trainingData_aws.append({'label':label,'text':text},ignore_index=True)\n",
        "  if i==trainingValues:\n",
        "    break;\n",
        "\n",
        "len(trainingData_aws.index)\n",
        "print(trainingData_aws)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3600000\n",
            "     label                                               text\n",
            "0        4  Stuning even for the non-gamer: This sound tra...\n",
            "1        4  The best soundtrack ever to anything.: I'm rea...\n",
            "2        4  Amazing!: This soundtrack is my favorite music...\n",
            "3        4  Excellent Soundtrack: I truly like this soundt...\n",
            "4        4  Remember, Pull Your Jaw Off The Floor After He...\n",
            "...    ...                                                ...\n",
            "9995     4  A revelation of life in small town America in ...\n",
            "9996     4  Great biography of a very interesting journali...\n",
            "9997     0  Interesting Subject; Poor Presentation: You'd ...\n",
            "9998     0  Don't buy: The box looked used and it is obvio...\n",
            "9999     4  Beautiful Pen and Fast Delivery.: The pen was ...\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TbbTpj9sRBS"
      },
      "source": [
        "Una vez creado el Panda.DataFrame introduciendo los datos del dataset en el formato desado, debemos eliminar stopwords y pasar a forma raiz los verbos conjugados y palabras en plural..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqD1YnBbdTnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1a7ff6-a479-4391-9a88-542b17d9e188"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "preprocessedText = []\n",
        "\n",
        "for row in trainingData_aws.itertuples():\n",
        "    \n",
        "    \n",
        "    text = word_tokenize(row[2]) ## indice de la columna que contiene el texto\n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]\n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    preprocessedText.append(text)\n",
        "\n",
        "preprocessedData_aws = trainingData_aws\n",
        "preprocessedData_aws['processed_text'] = preprocessedText\n",
        "\n",
        "len(preprocessedData_aws.index)\n",
        "print(preprocessedData_aws)\n",
        "preprocessedData_aws['label'].value_counts()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "     label  ...                                     processed_text\n",
            "0        4  ...  stune even thi sound track beauti It paint sen...\n",
            "1        4  ...  the best soundtrack ever anyth I read lot revi...\n",
            "2        4  ...  amaz thi soundtrack favorit music time hand th...\n",
            "3        4  ...  excel soundtrack I truli like soundtrack I enj...\n",
            "4        4  ...  rememb pull your jaw off the floor after hear ...\n",
            "...    ...  ...                                                ...\n",
            "9995     4  ...  A revel life small town america earli 1900 tho...\n",
            "9996     4  ...  great biographi interest journalist thi biogra...\n",
            "9997     0  ...  interest subject poor present you tell bore st...\n",
            "9998     0  ...  Do buy the box look use obvious new I tri cont...\n",
            "9999     4  ...  beauti pen fast deliveri the pen ship promptli...\n",
            "\n",
            "[10000 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5097\n",
              "4    4903\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsKVQbDsJke"
      },
      "source": [
        "A continuaci칩n creamos el Bag of Words a partir del conjunto de datos ya procesado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te13BjL4rw4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfa0c28-b97c-4e07-8e1c-7c65b6521748"
      },
      "source": [
        "\n",
        "bagOfWordsModel = TfidfVectorizer()\n",
        "bagOfWordsModel.fit(preprocessedData_aws['processed_text'])\n",
        "textsBoW_aws= bagOfWordsModel.transform(preprocessedData_aws['processed_text'])\n",
        "print(\"Finished\")\n",
        "\n",
        "print(textsBoW_aws.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished\n",
            "(10000, 21397)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9-7SI7-subs"
      },
      "source": [
        "Ahora entrenamos con Support Vector Machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jbDSnnEsF_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a387af7-7ea4-4cef-93e0-5a9028659665"
      },
      "source": [
        "svc_aws = svm.SVC(kernel='linear') #Modelo de clasificaci칩n\n",
        "\n",
        "X_train_aws = textsBoW_aws #Documentos\n",
        "Y_train_aws = trainingData_aws['label'] #Etiquetas de los documentos \n",
        "svc_aws.fit(X_train_aws, Y_train_aws) #Entrenamiento"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxsKv7EhtS6w"
      },
      "source": [
        "Ahora importamos el dataset de test (prueba) como hicimos con el anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPsvOf26tYSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b84a6ded-5207-40af-8d0c-8b6e6f364ae1"
      },
      "source": [
        "\n",
        "f = open(\"/content/drive/MyDrive/ESEI/7 Cuatrimestre/ABP/test.ft.txt\", \"r\")\n",
        "lines = f.readlines()\n",
        "testingData_aws = pd.DataFrame(columns=['label','text'])\n",
        "print(len(lines))\n",
        "i=0\n",
        "for line in lines:\n",
        "  i+=1\n",
        "  #print(\"\\t\",i)\n",
        "  label = line.split(' ',1)[0]\n",
        "  if label == \"__label__1\":\n",
        "    label=\"0\"\n",
        "  elif label == \"__label__2\":\n",
        "    label=\"4\"\n",
        "  #print(label)\n",
        "  text = line.split(' ',1)[1]\n",
        "  #print(text)\n",
        "  testingData_aws = testingData_aws.append({'label':label,'text':text},ignore_index=True)\n",
        "  if i==testingValues:\n",
        "    break;\n",
        "\n",
        "len(testingData_aws.index)\n",
        "print(testingData_aws)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000\n",
            "   label                                               text\n",
            "0      4  Great CD: My lovely Pat has one of the GREAT v...\n",
            "1      4  One of the best game music soundtracks - for a...\n",
            "2      0  Batteries died within a year ...: I bought thi...\n",
            "3      4  works fine, but Maha Energy is better: Check o...\n",
            "4      4  Great for the non-audiophile: Reviewed quite a...\n",
            "..   ...                                                ...\n",
            "95     4  The Scarlet Letter a must read for any America...\n",
            "96     0  The Scarlet Letter: A soap opera: Although thi...\n",
            "97     4  This book is Great!: One might notice that man...\n",
            "98     0  I had to read it...: I had to read this book f...\n",
            "99     4  Possibly the funniest movie ever made.: It sta...\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sotiv_gNaNZh"
      },
      "source": [
        "Seguidamente eliminamos stopwords y pasamos los datos por el stem para convertir las palabras a su raiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uA321h-twmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fab510-661e-4217-eb53-51555052b63a"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "preprocessedText = []\n",
        "\n",
        "for row in testingData_aws.itertuples():\n",
        "    \n",
        "    \n",
        "    text = word_tokenize(row[2]) ## indice de la columna que contiene el texto\n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]\n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    preprocessedText.append(text)\n",
        "\n",
        "preprocessedDataTest_aws = testingData_aws\n",
        "preprocessedDataTest_aws['processed_text'] = preprocessedText\n",
        "\n",
        "print(preprocessedDataTest_aws)\n",
        "print(testingData_aws['label'].value_counts())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label  ...                                     processed_text\n",
            "0      4  ...  great CD My love pat one great voic gener I li...\n",
            "1      4  ...  one best game music soundtrack game I realli p...\n",
            "2      0  ...  batteri die within year I bought charger jul 2...\n",
            "3      4  ...  work fine maha energi better check maha energi...\n",
            "4      4  ...  great review quit bit combo player hesit due u...\n",
            "..   ...  ...                                                ...\n",
            "95     4  ...  the scarlet letter must read american the best...\n",
            "96     0  ...  the scarlet letter A soap opera although book ...\n",
            "97     4  ...  thi book great one might notic mani neg review...\n",
            "98     0  ...  I read I read book school I like I sure point ...\n",
            "99     4  ...  possibl funniest movi ever made It start bit s...\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "4    53\n",
            "0    47\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9yo_CN4afGn"
      },
      "source": [
        "Creamos el Bag of Words con los textos de tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uJ55oecvCJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8be050-b8d8-4336-aa57-8404e5be8863"
      },
      "source": [
        "textsBoWTest_aws= bagOfWordsModel.transform(preprocessedDataTest_aws['processed_text'])\n",
        "print(\"Finished\")\n",
        "print(textsBoWTest_aws.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished\n",
            "(100, 21397)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R6wFneba8sZ"
      },
      "source": [
        "Generamos el array (ndarray, array de numPy) con las predicciones del svc entrenado anteriormente. Seguidamente empleamos el m칠todo classification_report para comparar las predicciones con las etiquetas reales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFEXEvE2vPUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee7eaea-15e5-4985-8741-078265132083"
      },
      "source": [
        "\n",
        "X_test_aws = textsBoWTest_aws #Documentos\n",
        "predictions_aws = svc_aws.predict(X_test_aws) #Se almacena en el array predictions las predicciones del clasificador\n",
        "Y_test_aws = testingData_aws['label'] #Etiquetas reales de los documentos\n",
        "\n",
        "print (classification_report(Y_test_aws, predictions_aws))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.91        47\n",
            "           4       0.92      0.91      0.91        53\n",
            "\n",
            "    accuracy                           0.91       100\n",
            "   macro avg       0.91      0.91      0.91       100\n",
            "weighted avg       0.91      0.91      0.91       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwW3szvxLbUv"
      },
      "source": [
        "# Segundo DS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH8d28smbho1"
      },
      "source": [
        "Para el segundo dataset repetiremos los pasos de la secci칩n anterior con peque침as diferencias:\n",
        "\n",
        "*   Importamos el .csv de la carpeta de drive\n",
        "*   Restringimos el tama침o del dataset para realizar pruebas.\n",
        "*   Agregamos el nombre de las columnas, y eliminamos las columnas innecesarias para quedarnos unicamente con el texto y la etiqueta.\n",
        "*   Procesamos los textos (tweets) eliminando stopwords y transformado las palabras a su forma raiz.\n",
        "*   Generamos el BagOfWords a partir del texto procesado.\n",
        "*   Entrenamos la svm con el nombre **svc_s140**.\n",
        "*   Repetimos los primeros 6 pasos para el dataset de prueba.\n",
        "*   Hacemos las predicciones con el dataset de prueba y la svc_s140. Seguidamente mostramos las predicciones para comprobar, entre otros, el accuracy obtenido.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axg4N2udLuDt",
        "outputId": "b16c43c6-3e49-4407-a7e2-e4ac155f1cfa"
      },
      "source": [
        "trainingData_s140 = pd.read_csv('/content/drive/MyDrive/ESEI/7 Cuatrimestre/ABP/Sentiment140/sentiment140.csv', encoding='ISO-8859-1')\n",
        "trainingData_s140 = trainingData_s140.head(trainingValues//2).append(trainingData_s140.tail(trainingValues//2))\n",
        "trainingData_s140 = trainingData_s140.set_axis(['label','id','date','flag','user','text'], axis=1, inplace=False)\n",
        "trainingData_s140 = trainingData_s140.drop(['id','date','flag','user'],axis=1)\n",
        "print(trainingData_s140)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         label                                               text\n",
            "0            0  is upset that he can't update his Facebook by ...\n",
            "1            0  @Kenichan I dived many times for the ball. Man...\n",
            "2            0    my whole body feels itchy and like its on fire \n",
            "3            0  @nationwideclass no, it's not behaving at all....\n",
            "4            0                      @Kwesidei not the whole crew \n",
            "...        ...                                                ...\n",
            "1599994      4  Just woke up. Having no school is the best fee...\n",
            "1599995      4  TheWDB.com - Very cool to hear old Walt interv...\n",
            "1599996      4  Are you ready for your MoJo Makeover? Ask me f...\n",
            "1599997      4  Happy 38th Birthday to my boo of alll time!!! ...\n",
            "1599998      4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shz94R8B93Lf",
        "outputId": "7288935b-5187-4c34-e7e1-766c6f7d541c"
      },
      "source": [
        "\n",
        "index_label = trainingData_s140[ trainingData_s140['label'] == \"2\" ].index\n",
        "\n",
        "trainingData_s140.drop(index_label, inplace = True)\n",
        "\n",
        "print(trainingData_s140)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         label                                               text\n",
            "0            0  is upset that he can't update his Facebook by ...\n",
            "1            0  @Kenichan I dived many times for the ball. Man...\n",
            "2            0    my whole body feels itchy and like its on fire \n",
            "3            0  @nationwideclass no, it's not behaving at all....\n",
            "4            0                      @Kwesidei not the whole crew \n",
            "...        ...                                                ...\n",
            "1599994      4  Just woke up. Having no school is the best fee...\n",
            "1599995      4  TheWDB.com - Very cool to hear old Walt interv...\n",
            "1599996      4  Are you ready for your MoJo Makeover? Ask me f...\n",
            "1599997      4  Happy 38th Birthday to my boo of alll time!!! ...\n",
            "1599998      4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMimScZIChjt",
        "outputId": "9f2a5f21-eda1-4782-9a0f-1fb84ab91c2e"
      },
      "source": [
        "preprocessedText = []\n",
        "\n",
        "for row in trainingData_s140.itertuples():\n",
        "    \n",
        "    \n",
        "    text = word_tokenize(row[2]) ## indice de la columna que contiene el texto\n",
        "    ## Remove stop words\n",
        "    #stops = set(stopwords.words(\"english\"))\n",
        "    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]\n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    preprocessedText.append(text)\n",
        "\n",
        "preprocessedData_s140 = trainingData_s140\n",
        "preprocessedData_s140['processed_text'] = preprocessedText\n",
        "\n",
        "len(preprocessedData_s140.index)\n",
        "print(preprocessedData_s140)\n",
        "preprocessedData_s140['label'].value_counts()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         label  ...                                     processed_text\n",
            "0            0  ...  upset ca updat facebook text might cri result ...\n",
            "1            0  ...  kenichan I dive mani time ball manag save 50 t...\n",
            "2            0  ...                    whole bodi feel itchi like fire\n",
            "3            0  ...                 nationwideclass behav mad I ca see\n",
            "4            0  ...                                kwesidei whole crew\n",
            "...        ...  ...                                                ...\n",
            "1599994      4  ...               just woke have school best feel ever\n",
            "1599995      4  ...             veri cool hear old walt interview http\n",
            "1599996      4  ...                   are readi mojo makeov ask detail\n",
            "1599997      4  ...  happi 38th birthday boo alll time tupac amaru ...\n",
            "1599998      4  ...  happi charitytuesday thenspcc sparkschar speak...\n",
            "\n",
            "[10000 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    5000\n",
              "0    5000\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7Zi21ssJcln",
        "outputId": "8342d792-5f21-496d-a01a-85fd335e998e"
      },
      "source": [
        "bagOfWordsModel = TfidfVectorizer()\n",
        "bagOfWordsModel.fit(preprocessedData_s140['processed_text'])\n",
        "textsBoW_s140= bagOfWordsModel.transform(preprocessedData_s140['processed_text'])\n",
        "print(\"Finished\")\n",
        "\n",
        "print(textsBoW_s140.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished\n",
            "(10000, 14369)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHHrRUWkKJa4"
      },
      "source": [
        "Ahora entrenamos la svm (Support Vector Machine)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o3dMLs0JsAu",
        "outputId": "20556605-e511-450e-dcf4-7eb2fdbb9dca"
      },
      "source": [
        "svc_s140 = svm.SVC(kernel='linear') #Modelo de clasificaci칩n\n",
        "\n",
        "X_train_s140 = textsBoW_s140 #Documentos\n",
        "Y_train_s140 = trainingData_s140['label'] #Etiquetas de los documentos \n",
        "svc_s140.fit(X_train_s140, Y_train_s140) #Entrenamiento"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEOIHucWKMgl"
      },
      "source": [
        "Ahora creamos el dataframe con los datos de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuMJIQVhKl63",
        "outputId": "39253363-f0ca-4efe-cbf2-7a2ae4915e69"
      },
      "source": [
        "testingData_s140 = pd.read_csv('/content/drive/MyDrive/ESEI/7 Cuatrimestre/ABP/Sentiment140/sentiment140.csv', encoding='ISO-8859-1')\n",
        "#testingData_s140 = testingData_s140.head(testingValues//2).append(testingData_s140.tail(testingValues//2))\n",
        "#testingData_s140 = testingData_s140.iloc[trainingValues//2:trainingValues//2+testingValues//2].append(testingData_s140.iloc[trainingValues//2+len(testingData_s140):trainingValues//2+testingValues//2]+len(testingData_s140))\n",
        "testingData_s140 = testingData_s140.iloc[trainingValues//2:trainingValues//2+testingValues//2].append(testingData_s140.iloc[1599449:1599499])\n",
        "\n",
        "testingData_s140 = testingData_s140.set_axis(['label','id','date','flag','user','text'], axis=1, inplace=False)\n",
        "testingData_s140 = testingData_s140.drop(['id','date','flag','user'],axis=1)\n",
        "\n",
        "print(testingData_s140)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         label                                               text\n",
            "5000         0         Orange juice and toothpaste mouth combo = \n",
            "5001         0  And hating that watching House will kill me! I...\n",
            "5002         0  Sitting in my car, waiting for my mum to come ...\n",
            "5003         0  @zampeachie @Farscale To answer both of you: I...\n",
            "5004         0  Is home! Been @ WCH since 8:30AM this morning....\n",
            "...        ...                                                ...\n",
            "1599494      4                    @Disney_Dreaming Mitchel it is \n",
            "1599495      4  just watched twilight...it was horrible...now ...\n",
            "1599496      4  @nayes1982 The changing them green in solidari...\n",
            "1599497      4  SUMMEEEEEEER!    Lines, Vines &amp; Trying tim...\n",
            "1599498      4  On the way to Walsrode! Tomorrow will be one o...\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ_zrqpbQTs9",
        "outputId": "57fe6b08-b38a-452b-acab-ce7dc7d07b2a"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "preprocessedText = []\n",
        "\n",
        "for row in testingData_s140.itertuples():\n",
        "    \n",
        "    \n",
        "    text = word_tokenize(row[2]) ## indice de la columna que contiene el texto\n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [ps.stem(w) for w in text if not w in stops and w.isalnum()]\n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    preprocessedText.append(text)\n",
        "\n",
        "preprocessedDataTest_s140 = testingData_s140\n",
        "preprocessedDataTest_s140['processed_text'] = preprocessedText\n",
        "\n",
        "print(preprocessedDataTest_s140)\n",
        "print(preprocessedDataTest_s140['label'].value_counts())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         label  ...                                     processed_text\n",
            "5000         0  ...                   orang juic toothpast mouth combo\n",
            "5001         0  ...  and hate watch hous kill I ca wait next episod...\n",
            "5002         0  ...                  sit car wait mum come home I lock\n",
            "5003         0  ...  zampeachi farscal To answer I studi four hour ...\n",
            "5004         0  ...  Is home been wch sinc morn been awak sinc 4 2n...\n",
            "...        ...  ...                                                ...\n",
            "1599494      4  ...                                            mitchel\n",
            "1599495      4  ...  watch twilight horribl I watch princess diari ...\n",
            "1599496      4  ...  nayes1982 the chang green solidar iranian iran...\n",
            "1599497      4  ...  summeeeeeeer line vine amp tri time mine sinc ...\n",
            "1599498      4  ...           On way walsrod tomorrow one greatest day\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "4    50\n",
            "0    50\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LXvdWrUQ5Ff",
        "outputId": "0899c845-149b-4c3d-e628-8b23be94de2f"
      },
      "source": [
        "textsBoWTest_s140 = bagOfWordsModel.transform(preprocessedDataTest['processed_text'])\n",
        "print(\"Finished\")\n",
        "print(textsBoWTest_s140.shape)\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished\n",
            "(100, 14369)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K0B98Hi_jyk"
      },
      "source": [
        "Generamos las predicciones con la svc entrenada anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNQgdVomQ6wH",
        "outputId": "8dd70294-b71d-4fc5-d061-1eb9414c15e1"
      },
      "source": [
        "X_test_s140 = textsBoWTest_s140 #Documentos\n",
        "\n",
        "predictions_s140 = svc_s140.predict(X_test_s140) #Se almacena en el array predictions las predicciones del clasificador\n",
        "\n",
        "Y_test_s140 = testingData_s140['label'] #Etiquetas reales de los documentos\n",
        "\n",
        "print(Y_test_s140)\n",
        "print(40*'*')\n",
        "print(predictions_s140)\n",
        "\n",
        "print(classification_report(Y_test_s140, predictions_s140))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000       0\n",
            "5001       0\n",
            "5002       0\n",
            "5003       0\n",
            "5004       0\n",
            "          ..\n",
            "1599494    4\n",
            "1599495    4\n",
            "1599496    4\n",
            "1599497    4\n",
            "1599498    4\n",
            "Name: label, Length: 100, dtype: int64\n",
            "****************************************\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 4 0 0 0 0 4 4 4 0 4 0 0 0 0\n",
            " 0 0 0 0 0 0 0 4 0 0 0 0 0 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 0 4 4 4 4 4 4 4 4 0 4 4 0 4 4 4 4 4 0 4 4 0 4 4 4]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88        50\n",
            "           4       0.87      0.90      0.88        50\n",
            "\n",
            "    accuracy                           0.88       100\n",
            "   macro avg       0.88      0.88      0.88       100\n",
            "weighted avg       0.88      0.88      0.88       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx25IdUvSLfG"
      },
      "source": [
        "#Pruebas cruzadas\n",
        "\n",
        "A continuaci칩n pasaremos a testear las dos SVMs con los datasets que NO fueron entrenados.\n",
        "Para el primer ejemplo hemos empleado el dataset de amazon reviews, que contiene las reviews de productos etiquetadas con positivo o negativo en funci칩n de como haya sido su review.\n",
        "\n",
        "Para el segundo dataset hemos empleado el dataset de sentiment140, empleado en las clases de teor칤a, que contiene millones de tweets etiquetados en funci칩n de su connotaci칩n (negativa, neutra o positiva). Nosotros hemos eliminado las publicaciones neutras para que se adecuara al entrenamiento del primer caso y as칤 poder despues compararlos.\n",
        "\n",
        "Nombres del dataset de Amazon Reviews:\n",
        "\n",
        "\n",
        "*   trainingData_aws : DataFrame del dataset de entrenamiento de amazon reviews\n",
        "*   testingData_aws : DataFrame del dataset de pruebas de amazon reviews\n",
        "*   TextsBoW_aws : Bag Of Words del dataset de entrenamiento de amazon reviews\n",
        "*   TextsBoWTest_aws : Bag Of Words del dataset de pruebas de amazon reviews\n",
        "*   svc_aws : SVM entrenada con el dataset de amazon reviews\n",
        "*   X_train_aws : Textos del dataset de entrenamiento de amazon reviews\n",
        "*   Y_train_aws : Etiquetas dataset de entrenamiento de amazon reviews\n",
        "*   X_test_aws : Textos del dataset de pruebas de amazon reviews\n",
        "*   Y_test_aws : Etiquetas del dataset de pruebas de amazon reviews\n",
        "\n",
        "Nombres del dataset de Sentiment 140:\n",
        "*   trainingData_s140 : DataFrame del dataset de entrenamiento de sentiment 140\n",
        "*   testingData_s140 : DataFrame del dataset de pruebas de sentiment 140\n",
        "*   TextsBoW_s140 : Bag Of Words del dataset de entrenamiento de sentiment 140\n",
        "*   TextsBoWTest_s140 : Bag Of Words del dataset de pruebas de sentiment 140\n",
        "*   svc_s140 : SVM entrenada con el dataset de sentiment 140\n",
        "*   X_train_s140 : Textos del dataset de entrenamiento de sentiment 140\n",
        "*   Y_train_s140 : Etiquetas dataset de entrenamiento de sentiment 140\n",
        "*   X_test_s140 : Textos del dataset de pruebas de sentiment 140\n",
        "*   Y_test_s140 : Etiquetas del dataset de pruebas de sentiment 140\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bv6PN-MFMqO"
      },
      "source": [
        "Ahora nos creamos un bag of words que contenga todos los textos de los dos datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOqVWzkUHO4b",
        "outputId": "79d6665d-af3f-46b8-e52a-1ee76bc1bc3f"
      },
      "source": [
        "bagOfWordsModel = TfidfVectorizer()\n",
        "preprocessedData_aws_s140 = preprocessedData_aws.append(preprocessedData_s140)\n",
        "print(preprocessedData_aws_s140.shape)\n",
        "bagOfWordsModel.fit(preprocessedData_aws_s140['processed_text'])\n",
        "textsBoW_aws_s140= bagOfWordsModel.transform(preprocessedData_aws_s140['processed_text'])\n",
        "print(\"Finished\")\n",
        "\n",
        "print(textsBoW_aws_s140.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 3)\n",
            "Finished\n",
            "(20000, 29877)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "j2ITy76ISoot",
        "outputId": "05ac87d5-1624-4efd-90fe-7cc563a0b08e"
      },
      "source": [
        "X_test_aws_s140 = textsBoW_aws_s140 #Documentos\n",
        "\n",
        "predictions_aws_s140 = svc_s140.predict(X_test_aws_s140) #Se almacena en el array predictions las predicciones del clasificador\n",
        "\n",
        "Y_test_aws_s140 = testingData_aws['label'].append(testingData_s140['label']) #Etiquetas reales de los documentos\n",
        "\n",
        "print(X_test_aws_s140)\n",
        "print(40*'*')\n",
        "print(predictions_aws_s140)\n",
        "\n",
        "print(classification_report(Y_test_aws_s140, predictions_aws_s140))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-93e4dd679d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_test_aws_s140\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextsBoW_aws_s140\u001b[0m \u001b[0;31m#Documentos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions_aws_s140\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc_s140\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_aws_s140\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Se almacena en el array predictions las predicciones del clasificador\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_test_aws_s140\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestingData_aws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestingData_s140\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Etiquetas reales de los documentos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    465\u001b[0m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[1;32m    466\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                              (n_features, self.shape_fit_[1]))\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 29877 should be equal to 14369, the number of features at training time"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W_TjCd8_h0Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}